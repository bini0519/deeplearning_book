{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4장. 머신러닝 기초 이론들.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM/bH5T2+0kwBsdIbEWP60+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"N3jcdJb37v6K"},"source":["<목차>\r\n","---\r\n","1. Gradient Descent\r\n","2. Training, Validation, Test Data & Overfitting\r\n","3. Softmax Regression\r\n","    * 소프트맥스 회귀\r\n","    * 크로스 엔트로피 손실 함수\r\n","    * MNIST 데이터셋\r\n","    * One-hot Encoding\r\n","4. Softmax Regression을 이용한 MNIST 숫자 분류기 구현"]},{"cell_type":"markdown","metadata":{"id":"GuezMEuZ83xx"},"source":["# 1. Gradient Descent (경사하강법)\r\n","---\r\n","손실 함수를 최소화하는 대표적인 알고리즘  \r\n","- Batch Gradient Descent  \r\n","    - **전체** traing set을 하나의 batch로 만듬  \r\n","    - 단점: 시간이 오래 걸림\r\n","- Stochastic Gradient Descent   \r\n","    - 한 스텝 진행할 때 **1개**의 training set만 사용  \r\n","    - 단점: 파라미터를 한 번 업데이트할 때, 전체 set의 특성을 고려하지 않음  \r\n","- Mini-Batch Gradient Descent\r\n","    - 위 단점들을 해결  \r\n","    ex) 전체 set이 1000개라면 100개씩 mini-batch를 만들어 파라미터를 업데이트\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"eeUiOweO_q0g"},"source":["# 2. Training, Validation, Test Data & Overfitting\r\n","---\r\n","# 3. Softmax Regression\r\n","---\r\n","- Softmax Regression  \r\n","  - 레이블을 분류하기 위한 목적  \r\n","  - Softmax 함수는 정규화함수로써 출력값들의 합을 1로 만든다.  \r\n","  - 모델의 출력값이 label에 대한 확률을 나타낸다.  \r\n","\r\n","- Cross-Entropy 손실 함수\r\n","  - 분류 문제에 많이 사용됨\r\n","\r\n","- MNIST 데이터셋\r\n","  - 60,000장의 training set, 10,000장의 test set  \r\n","  - 0~9사이의 28*28 크기의 필기체 이미지로 구성\r\n","\r\n","- One-hot Encoding\r\n","  - 범주형 값을 이진화된 값으로 바꿔서 표현"]},{"cell_type":"markdown","metadata":{"id":"Ps21lEY46vwp"},"source":["# 4. Softmax Regression을 이용한 MNIST 숫자 분류기 구현 \r\n","---\r\n","데이터 다운로드"]},{"cell_type":"code","metadata":{"id":"lhPmK07o5XgZ"},"source":["#텐서플로 버전2\r\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWIhkQT25u_T"},"source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n","\r\n","# 이미지들을 float32 데이터 타입으로 변경\r\n","x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\r\n","\r\n","# 28*28 형태의 이미지를 784차원으로 flattening \r\n","x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\r\n","\r\n","# [0, 255] 사이의 값을 [0, 1]사이의 값으로 Normalize\r\n","x_train, x_test = x_train / 255., x_test / 255.\r\n","\r\n","# 레이블 데이터에 one-hot encoding을 적용\r\n","y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\r\n","\r\n","# train data\r\n","# tf.data API를 이용해서 데이터를 섞고 batch 형태로 가져옵니다.\r\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n","train_data = train_data.repeat().shuffle(60000).batch(100)\r\n","train_data_iter = iter(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3BgxRCs6zpu"},"source":["모델 정의"]},{"cell_type":"code","metadata":{"id":"60oD3PsQ5yh3"},"source":["class SoftmaxRegression(tf.keras.Model):\r\n","  def __init__(self):\r\n","    super(SoftmaxRegression, self).__init__()\r\n","    self.softmax_layer = tf.keras.layers.Dense(10,\r\n","                                               activation=None,\r\n","                                               kernel_initializer='zeros',\r\n","                                               bias_initializer='zeros')\r\n","\r\n","  def call(self, x):\r\n","    logits = self.softmax_layer(x)\r\n","\r\n","    return tf.nn.softmax(logits)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U37CRxZ-7Eil"},"source":["손실 함수 정의"]},{"cell_type":"code","metadata":{"id":"6yfiAuot66tV"},"source":["def cross_entropy_loss(y_pred, y):\r\n","  return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(y_pred), axis=[1]))\r\n","   #return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logtis, labels=y)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9FpzwFzqTO4E"},"source":["다른 방법: TensorFlow API를 이용한 구현  \r\n","**tf.nn.softmax_cross_entropy_with_logits_v2**(labels = None, logits = None, name = None)  \r\n","- labels: 정답 레이블  \r\n","- logits: softmax 함수를 적용해서 정규화하기 전 모델의 출력값 logits\r\n"]},{"cell_type":"markdown","metadata":{"id":"tKE3quc97OUW"},"source":["최적화 함수 정의"]},{"cell_type":"code","metadata":{"id":"tG82B7G67NB0"},"source":["optimizer = tf.optimizers.SGD(0.5)\r\n","\r\n","def train_step(model, x, y):\r\n","  with tf.GradientTape() as tape:\r\n","    y_pred = model(x)\r\n","    loss = cross_entropy_loss(y_pred, y)\r\n","  gradients = tape.gradient(loss, model.trainable_variables)\r\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zte2hr0g7TI8"},"source":["모델 성능 출력 함수"]},{"cell_type":"code","metadata":{"id":"WXWM-EGZ7Rw7"},"source":["def compute_accuracy(y_pred, y):\r\n","  correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\r\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZQfwg4f7YZJ"},"source":["실행"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3TitdR07XQ7","executionInfo":{"status":"ok","timestamp":1610175335065,"user_tz":-540,"elapsed":2713,"user":{"displayName":"임수빈","photoUrl":"https://lh4.googleusercontent.com/-shQjEu-1d-U/AAAAAAAAAAI/AAAAAAAAALQ/0wWCQNge2WI/s64/photo.jpg","userId":"18239898010736387958"}},"outputId":"3cb0bd2e-c007-405c-bd3b-ff63c733757f"},"source":["# SoftmaxRegression 모델을 선언\r\n","SoftmaxRegression_model = SoftmaxRegression()\r\n","\r\n","# 1000번 반복을 수행하면서 최적화를 수행\r\n","for i in range(1000):\r\n","  batch_xs, batch_ys = next(train_data_iter)\r\n","  train_step(SoftmaxRegression_model, batch_xs, batch_ys)\r\n","\r\n","# 학습이 끝나면 학습된 모델의 정확도를 출력\r\n","print(\"정확도(Accuracy): %f\" % compute_accuracy(SoftmaxRegression_model(x_test), y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["정확도(Accuracy): 0.920500\n"],"name":"stdout"}]}]}