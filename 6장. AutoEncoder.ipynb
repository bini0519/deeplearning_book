{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6장. AutoEncoder.ipynb","provenance":[],"authorship_tag":"ABX9TyPUbn+bxoqs0KLRCLkp6XyM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NA6lSNnoPSXd"},"source":["<목차>\r\n","---\r\n","1. AutoEncoder의 개념  \r\n","2. MNIST 데이터 재구축\r\n","3. MNIST 분류기 구현  \r\n","  - 파인 튜닝과 전이 학습\r\n","  - MNIST 분류기 구현"]},{"cell_type":"markdown","metadata":{"id":"SlFjTguSh7RM"},"source":["# 1. AutoEncoder의 개념\r\n","---\r\n","- 비지도 학습을 위한 인공신경망 구조  \r\n","- 오토인코더의 출력은 원본 데이터를 **재구축**(Reconstruction)한 결과  \r\n","- 은닉층의 출력값은 원본 데이터에서 불필요한 특징들을 제거한 **압축된 특징**"]},{"cell_type":"markdown","metadata":{"id":"SkYZW1iZk32j"},"source":["# 2. MNIST 데이터 재구축\r\n","---"]},{"cell_type":"code","metadata":{"id":"8Tn55gxSlsUo"},"source":["!pip install tensorflow==1.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5d46VvewPPaM"},"source":["import tensorflow as tf\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6uqGPVLlTxZ"},"source":["#데이터 다운로드\r\n","from tensorflow.examples.tutorials.mnist import input_data\r\n","mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEZzltBM-5VW"},"source":["#학습에 필요한 설정값 정의\r\n","learning_rate = 0.02\r\n","training_epochs = 20\r\n","batch_size = 256\r\n","display_step = 1 #손실함수 출력 주기\r\n","examples_to_show = 10 #보여줄 재구축 이미지\r\n","input_size = 784\r\n","hidden1_size = 256\r\n","hidden2_size = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVjsf9Vnlkhu","executionInfo":{"status":"ok","timestamp":1610441454672,"user_tz":-540,"elapsed":666,"user":{"displayName":"임수빈","photoUrl":"https://lh4.googleusercontent.com/-shQjEu-1d-U/AAAAAAAAAAI/AAAAAAAAALQ/0wWCQNge2WI/s64/photo.jpg","userId":"18239898010736387958"}}},"source":["#입력값 받기\r\n","x = tf.placeholder(tf.float32, shape=[None, input_size])\r\n","\r\n","#-------------------------------------모델 정의--------------------------------------------\r\n","def build_autoencoder(x):\r\n","  #인코딩 784 > 256 > 128\r\n","  W1 = tf.Variable(tf.random_normal(shape=[input_size, hidden1_size]))\r\n","  b1 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\r\n","  H1_output = tf.nn.sigmoid(tf.matmul(x,W1) + b1)\r\n","\r\n","  W2 = tf.Variable(tf.random_normal(shape=[hidden1_size, hidden2_size]))\r\n","  b2 = tf.Variable(tf.random_normal(shape=[hidden2_size]))\r\n","  H2_output = tf.nn.sigmoid(tf.matmul(H1_output,W2) + b2)\r\n","  \r\n","  #디코딩 128 > 256 > 784\r\n","  W3 = tf.Variable(tf.random_normal(shape=[hidden2_size, hidden1_size]))\r\n","  b3 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\r\n","  H3_output = tf.nn.sigmoid(tf.matmul(H2_output,W3) + b3)\r\n","\r\n","  W4 = tf.Variable(tf.random_normal(shape=[hidden1_size, input_size]))\r\n","  b4 = tf.Variable(tf.random_normal(shape=[input_size]))\r\n","  reconstructed_x = tf.nn.sigmoid(tf.matmul(H3_output,W4) + b4)\r\n","\r\n","  return reconstructed_x\r\n","\r\n","#---------------------------------그래프 구조 생성--------------------------------\r\n","y_pred = build_autoencoder(x)\r\n","\r\n","#-------------------------손실함수와 옵티마이저 정의------------------------------\r\n","loss = tf.reduce_mean(tf.pow(x - y_pred, 2))\r\n","train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\r\n","\r\n","#------------------------------------그래프 실행-----------------------------------\r\n","with tf.Session() as sess:\r\n","  sess.run(tf.global_variables_initializer())\r\n","\r\n","  #최적화 반복\r\n","  for epoch in range(training_epochs):\r\n","    #전체 배치 불러오기\r\n","    total_batch = int(mnist.train.num_examples / batch_size)\r\n","    #각 배치에 최적화 수행\r\n","    for i in range(total_batch):\r\n","      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n","      _, current_loss = sess.run([training_epochs, loss], feed_dict={x:batch_xs})\r\n","\r\n","    #학습결과 출력\r\n","    if epoch % display_step == 0:\r\n","      print(\"반복: %d, 손실함수: %f\" % ((epoch+1), current_loss))\r\n","\r\n","  \r\n","  #test데이터 재구축\r\n","  reconstructed_result = sess.run(y_pred, feed_dict={x:mnist.test.images[:examples_to_show]})\r\n","\r\n","  #test데이터 vs 재구축 결과\r\n","  f, a = plt.subplots(2, 10, figsize=(10, 2))\r\n","  for i in range(examples_to_show):\r\n","    a[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))\r\n","    a[1][i].imshow(np.reshape(reconstructed_result[i], (28,28)))\r\n","  f.savfig('reconstructed_mnist_image.png')\r\n","  f.show()\r\n","  plt.draw()\r\n","  plt.waitforbuttonpress()"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqNREWC6zNNf"},"source":["# 3. MNIST 분류기 구현\r\n","---\r\n","- 파인튜닝(Fine-Tuning)과 전이학습(Transfer learning)\r\n","  - B 문제를 풀기 위한 모델을 학습시키고자 할 때, A 문제를 풀기 위해 학습된 파라미터를 가져와서 수정하는 기법\r\n","  - AutoEncoder의 디코딩 부분을 삭제하고 인코딩된 특징값을 분류기의 입력값으로 학습  \r\n","  - 분류 성능을 더 높이는 효과\r\n","- 분류기 학습 순서  \r\n","1) **사전 학습**: 데이터 재구축 목적으로 AutoEncoder 학습  \r\n","2) **파인 튜닝**: 숫자 분류를 목적으로 다시 최적화하기 위해 AutoEncoder 학습"]},{"cell_type":"code","metadata":{"id":"KC7t0-bAyoqL","executionInfo":{"status":"ok","timestamp":1610454131773,"user_tz":-540,"elapsed":725,"user":{"displayName":"임수빈","photoUrl":"https://lh4.googleusercontent.com/-shQjEu-1d-U/AAAAAAAAAAI/AAAAAAAAALQ/0wWCQNge2WI/s64/photo.jpg","userId":"18239898010736387958"}}},"source":["#학습에 필요한 설정값 정의\r\n","learning_rate_RMSProp = 0.02\r\n","learning_rate_GradientDescent = 0.5 \r\n","num_epochs = 100\r\n","batch_size = 256\r\n","display_step = 1\r\n","input_size = 784\r\n","hidden1_size = 128\r\n","hidden2_size = 64 \r\n","#설정값 아래 코드에 직접 대입"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lAHLhUr54XS","outputId":"99ac3506-b972-4dc1-d980-fc1ce4dddf86"},"source":["x = tf.placeholder(tf.float32, shape=[None, input_size])\r\n","y = tf.placeholder(tf.float32, shape=[None, 10]) #실제 MNIST 숫자값\r\n","\r\n","#--------------------------------AutoEncoder 구조 정의-----------------------------------------------\r\n","def build_autoencoder(x):\r\n","  #인코딩 784 > 256 > 128\r\n","  W1 = tf.Variable(tf.random_normal(shape=[input_size, hidden1_size]))\r\n","  b1 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\r\n","  H1_output = tf.nn.sigmoid(tf.matmul(x,W1) + b1)\r\n","\r\n","  W2 = tf.Variable(tf.random_normal(shape=[hidden1_size, hidden2_size]))\r\n","  b2 = tf.Variable(tf.random_normal(shape=[hidden2_size]))\r\n","  H2_output = tf.nn.sigmoid(tf.matmul(H1_output,W2) + b2)\r\n","  \r\n","  #디코딩 128 > 256 > 784\r\n","  W3 = tf.Variable(tf.random_normal(shape=[hidden2_size, hidden1_size]))\r\n","  b3 = tf.Variable(tf.random_normal(shape=[hidden1_size]))\r\n","  H3_output = tf.nn.sigmoid(tf.matmul(H2_output,W3) + b3)\r\n","\r\n","  W4 = tf.Variable(tf.random_normal(shape=[hidden1_size, input_size]))\r\n","  b4 = tf.Variable(tf.random_normal(shape=[input_size]))\r\n","  reconstructed_x = tf.nn.sigmoid(tf.matmul(H3_output,W4) + b4)\r\n","\r\n","  return reconstructed_x, H2_output\r\n","\r\n","#------------------------------Softmax 분류기 정의------------------------------------------------------\r\n","def build_softmax_classifier(x):\r\n","  \r\n","  #원본 이미지(784) 대신 재구축된 특징(64)을 입력값으로 받음\r\n","  W_softmax = tf.Variable(tf.zeros([hidden2_size, 10])) \r\n","  b_softmax = tf.Variable(tf.zeros([10]))\r\n","  y_pred = tf.nn.softmax(tf.matmul(x, W_softmax) + b_softmax)\r\n","\r\n","  return y_pred\r\n","\r\n","#----------------------------------그래프 구조 생성------------------------------------------------------\r\n","y_pred, extracted_features = build_autoencoder(x)\r\n","y_pred_softmax = build_softmax_classifier(extracted_features)\r\n","\r\n","#----------------------------------1) 사전 학습----------------------------------------------------------\r\n","pretraining_loss = tf.reduce_mean(tf.pow(x - y_pred, 2))\r\n","pretraining_train_step = tf.train.RMSPropOptimizer(learning_rate_RMSProp).minimize(pretraining_loss)\r\n","\r\n","#-------------------------2) 파인 튜닝(손실함수와 옵티마이저 정의)---------------------------------------\r\n","finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred_softmax), reduction_indices=[1]))\r\n","finetuning_train_step = tf.train.GradientDescentOptimizer(learning_rate_GradientDescent).minimize(finetuning_loss)\r\n","\r\n","#----------------------------------------그래프 실행-----------------------------------------------------\r\n","with tf.Session() as sess:\r\n","  sess.run(tf.global_variables_initializer())\r\n","\r\n","  total_batch = int(mnist.train.num_examples/batch_size)\r\n","\r\n","  #사전학습 최적화\r\n","  for epoch in range(num_epochs):\r\n","    for i in range(total_batch):\r\n","      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n","      _, pretraining_loss_print = sess.run([pretraining_train_step, pretraining_loss], feed_dict={x:batch_xs})\r\n","    if epoch % display_step == 0:\r\n","      print(\"반복: %d, 사전학습 손실함수: %f\" %((epoch+1), pretraining_loss_print))\r\n","  print(\"사전학습 최적화 완료\")\r\n","\r\n","  #파인튜닝 최적화\r\n","  for epoch in range(num_epochs+100):\r\n","    for i in range(total_batch):\r\n","      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n","      _, finetuning_loss_print = sess.run([finetuning_train_step, finetuning_loss], feed_dict={x:batch_xs, y:batch_ys})\r\n","    if epoch % display_step == 0:\r\n","      print(\"반복: %d, 파인튜닝 손실함수: %f\" %((epoch+1), finetuning_loss_print))\r\n","  print(\"파인튜닝 최적화 완료\")\r\n","\r\n","  #정확도 출력\r\n","  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred_softmax, 1))\r\n","  accuracy = tf.reduce_mean(tf.cast(correct_predicion, tf.float32))\r\n","  print(\"정확도: %f\" %sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["반복: 1, 사전학습 손실함수: 0.173456\n","반복: 2, 사전학습 손실함수: 0.107189\n","반복: 3, 사전학습 손실함수: 0.094686\n","반복: 4, 사전학습 손실함수: 0.080522\n","반복: 5, 사전학습 손실함수: 0.067303\n","반복: 6, 사전학습 손실함수: 0.063146\n","반복: 7, 사전학습 손실함수: 0.059718\n","반복: 8, 사전학습 손실함수: 0.057705\n","반복: 9, 사전학습 손실함수: 0.054638\n","반복: 10, 사전학습 손실함수: 0.051806\n","반복: 11, 사전학습 손실함수: 0.050090\n","반복: 12, 사전학습 손실함수: 0.049157\n","반복: 13, 사전학습 손실함수: 0.046456\n","반복: 14, 사전학습 손실함수: 0.047479\n","반복: 15, 사전학습 손실함수: 0.047068\n","반복: 16, 사전학습 손실함수: 0.045666\n","반복: 17, 사전학습 손실함수: 0.042720\n","반복: 18, 사전학습 손실함수: 0.041650\n","반복: 19, 사전학습 손실함수: 0.040096\n","반복: 20, 사전학습 손실함수: 0.040642\n","반복: 21, 사전학습 손실함수: 0.040900\n","반복: 22, 사전학습 손실함수: 0.040152\n","반복: 23, 사전학습 손실함수: 0.040299\n","반복: 24, 사전학습 손실함수: 0.039190\n","반복: 25, 사전학습 손실함수: 0.036930\n","반복: 26, 사전학습 손실함수: 0.036297\n","반복: 27, 사전학습 손실함수: 0.034307\n","반복: 28, 사전학습 손실함수: 0.033592\n","반복: 29, 사전학습 손실함수: 0.033702\n","반복: 30, 사전학습 손실함수: 0.031986\n","반복: 31, 사전학습 손실함수: 0.032740\n","반복: 32, 사전학습 손실함수: 0.032521\n","반복: 33, 사전학습 손실함수: 0.031337\n","반복: 34, 사전학습 손실함수: 0.030314\n","반복: 35, 사전학습 손실함수: 0.032664\n","반복: 36, 사전학습 손실함수: 0.031706\n","반복: 37, 사전학습 손실함수: 0.028362\n","반복: 38, 사전학습 손실함수: 0.029590\n","반복: 39, 사전학습 손실함수: 0.029549\n","반복: 40, 사전학습 손실함수: 0.027805\n","반복: 41, 사전학습 손실함수: 0.031081\n","반복: 42, 사전학습 손실함수: 0.026962\n","반복: 43, 사전학습 손실함수: 0.029458\n","반복: 44, 사전학습 손실함수: 0.027696\n","반복: 45, 사전학습 손실함수: 0.030562\n","반복: 46, 사전학습 손실함수: 0.029955\n","반복: 47, 사전학습 손실함수: 0.028289\n","반복: 48, 사전학습 손실함수: 0.028250\n","반복: 49, 사전학습 손실함수: 0.025266\n","반복: 50, 사전학습 손실함수: 0.026348\n","반복: 51, 사전학습 손실함수: 0.023486\n","반복: 52, 사전학습 손실함수: 0.024118\n","반복: 53, 사전학습 손실함수: 0.024169\n","반복: 54, 사전학습 손실함수: 0.026312\n","반복: 55, 사전학습 손실함수: 0.024527\n","반복: 56, 사전학습 손실함수: 0.023842\n","반복: 57, 사전학습 손실함수: 0.023973\n","반복: 58, 사전학습 손실함수: 0.025420\n","반복: 59, 사전학습 손실함수: 0.024435\n","반복: 60, 사전학습 손실함수: 0.023897\n","반복: 61, 사전학습 손실함수: 0.025580\n","반복: 62, 사전학습 손실함수: 0.023535\n","반복: 63, 사전학습 손실함수: 0.024415\n","반복: 64, 사전학습 손실함수: 0.023284\n","반복: 65, 사전학습 손실함수: 0.023829\n","반복: 66, 사전학습 손실함수: 0.021953\n","반복: 67, 사전학습 손실함수: 0.023027\n","반복: 68, 사전학습 손실함수: 0.022621\n","반복: 69, 사전학습 손실함수: 0.023192\n","반복: 70, 사전학습 손실함수: 0.022597\n","반복: 71, 사전학습 손실함수: 0.021539\n","반복: 72, 사전학습 손실함수: 0.022502\n","반복: 73, 사전학습 손실함수: 0.023117\n","반복: 74, 사전학습 손실함수: 0.023556\n","반복: 75, 사전학습 손실함수: 0.023602\n","반복: 76, 사전학습 손실함수: 0.021853\n","반복: 77, 사전학습 손실함수: 0.022036\n","반복: 78, 사전학습 손실함수: 0.020857\n","반복: 79, 사전학습 손실함수: 0.020908\n","반복: 80, 사전학습 손실함수: 0.019348\n","반복: 81, 사전학습 손실함수: 0.019805\n","반복: 82, 사전학습 손실함수: 0.022907\n","반복: 83, 사전학습 손실함수: 0.018828\n","반복: 84, 사전학습 손실함수: 0.019286\n","반복: 85, 사전학습 손실함수: 0.018952\n","반복: 86, 사전학습 손실함수: 0.019804\n","반복: 87, 사전학습 손실함수: 0.018710\n","반복: 88, 사전학습 손실함수: 0.019769\n","반복: 89, 사전학습 손실함수: 0.020763\n","반복: 90, 사전학습 손실함수: 0.019819\n","반복: 91, 사전학습 손실함수: 0.019655\n","반복: 92, 사전학습 손실함수: 0.018408\n","반복: 93, 사전학습 손실함수: 0.019588\n","반복: 94, 사전학습 손실함수: 0.017522\n","반복: 95, 사전학습 손실함수: 0.018413\n","반복: 96, 사전학습 손실함수: 0.018557\n","반복: 97, 사전학습 손실함수: 0.017345\n","반복: 98, 사전학습 손실함수: 0.017715\n","반복: 99, 사전학습 손실함수: 0.017563\n","반복: 100, 사전학습 손실함수: 0.016604\n","사전학습 최적화 완료\n","반복: 1, 파인튜닝 손실함수: 0.518729\n","반복: 2, 파인튜닝 손실함수: 0.375965\n","반복: 3, 파인튜닝 손실함수: 0.280241\n","반복: 4, 파인튜닝 손실함수: 0.288404\n","반복: 5, 파인튜닝 손실함수: 0.245977\n","반복: 6, 파인튜닝 손실함수: 0.221475\n","반복: 7, 파인튜닝 손실함수: 0.314789\n","반복: 8, 파인튜닝 손실함수: 0.173976\n","반복: 9, 파인튜닝 손실함수: 0.190892\n","반복: 10, 파인튜닝 손실함수: 0.159781\n","반복: 11, 파인튜닝 손실함수: 0.183188\n","반복: 12, 파인튜닝 손실함수: 0.161964\n","반복: 13, 파인튜닝 손실함수: 0.159676\n","반복: 14, 파인튜닝 손실함수: 0.146726\n","반복: 15, 파인튜닝 손실함수: 0.129863\n","반복: 16, 파인튜닝 손실함수: 0.156531\n","반복: 17, 파인튜닝 손실함수: 0.210019\n","반복: 18, 파인튜닝 손실함수: 0.183348\n","반복: 19, 파인튜닝 손실함수: 0.157200\n","반복: 20, 파인튜닝 손실함수: 0.142431\n","반복: 21, 파인튜닝 손실함수: 0.130305\n","반복: 22, 파인튜닝 손실함수: 0.151559\n","반복: 23, 파인튜닝 손실함수: 0.144383\n","반복: 24, 파인튜닝 손실함수: 0.088830\n","반복: 25, 파인튜닝 손실함수: 0.108432\n","반복: 26, 파인튜닝 손실함수: 0.099529\n","반복: 27, 파인튜닝 손실함수: 0.125858\n","반복: 28, 파인튜닝 손실함수: 0.192721\n","반복: 29, 파인튜닝 손실함수: 0.119901\n","반복: 30, 파인튜닝 손실함수: 0.152012\n","반복: 31, 파인튜닝 손실함수: 0.142560\n","반복: 32, 파인튜닝 손실함수: 0.122201\n","반복: 33, 파인튜닝 손실함수: 0.115959\n","반복: 34, 파인튜닝 손실함수: 0.051907\n","반복: 35, 파인튜닝 손실함수: 0.158303\n","반복: 36, 파인튜닝 손실함수: 0.120561\n","반복: 37, 파인튜닝 손실함수: 0.131137\n","반복: 38, 파인튜닝 손실함수: 0.102216\n","반복: 39, 파인튜닝 손실함수: 0.096914\n","반복: 40, 파인튜닝 손실함수: 0.101001\n","반복: 41, 파인튜닝 손실함수: 0.167086\n","반복: 42, 파인튜닝 손실함수: 0.127753\n","반복: 43, 파인튜닝 손실함수: 0.092493\n","반복: 44, 파인튜닝 손실함수: 0.095968\n","반복: 45, 파인튜닝 손실함수: 0.056378\n","반복: 46, 파인튜닝 손실함수: 0.088476\n","반복: 47, 파인튜닝 손실함수: 0.106651\n","반복: 48, 파인튜닝 손실함수: 0.121249\n","반복: 49, 파인튜닝 손실함수: 0.088172\n","반복: 50, 파인튜닝 손실함수: 0.085624\n","반복: 51, 파인튜닝 손실함수: 0.039001\n","반복: 52, 파인튜닝 손실함수: 0.060664\n","반복: 53, 파인튜닝 손실함수: 0.059339\n","반복: 54, 파인튜닝 손실함수: 0.092855\n","반복: 55, 파인튜닝 손실함수: 0.110218\n","반복: 56, 파인튜닝 손실함수: 0.035235\n","반복: 57, 파인튜닝 손실함수: 0.052856\n","반복: 58, 파인튜닝 손실함수: 0.059475\n","반복: 59, 파인튜닝 손실함수: 0.040943\n","반복: 60, 파인튜닝 손실함수: 0.045140\n","반복: 61, 파인튜닝 손실함수: 0.048960\n","반복: 62, 파인튜닝 손실함수: 0.073839\n","반복: 63, 파인튜닝 손실함수: 0.053029\n","반복: 64, 파인튜닝 손실함수: 0.051246\n","반복: 65, 파인튜닝 손실함수: 0.027351\n","반복: 66, 파인튜닝 손실함수: 0.076287\n","반복: 67, 파인튜닝 손실함수: 0.041897\n","반복: 68, 파인튜닝 손실함수: 0.050145\n","반복: 69, 파인튜닝 손실함수: 0.027162\n","반복: 70, 파인튜닝 손실함수: 0.083110\n","반복: 71, 파인튜닝 손실함수: 0.031387\n","반복: 72, 파인튜닝 손실함수: 0.033164\n","반복: 73, 파인튜닝 손실함수: 0.057840\n","반복: 74, 파인튜닝 손실함수: 0.057229\n","반복: 75, 파인튜닝 손실함수: 0.032444\n","반복: 76, 파인튜닝 손실함수: 0.037500\n","반복: 77, 파인튜닝 손실함수: 0.027611\n","반복: 78, 파인튜닝 손실함수: 0.048757\n","반복: 79, 파인튜닝 손실함수: 0.029247\n","반복: 80, 파인튜닝 손실함수: 0.062782\n","반복: 81, 파인튜닝 손실함수: 0.056270\n","반복: 82, 파인튜닝 손실함수: 0.052598\n","반복: 83, 파인튜닝 손실함수: 0.074009\n","반복: 84, 파인튜닝 손실함수: 0.045913\n","반복: 85, 파인튜닝 손실함수: 0.037339\n","반복: 86, 파인튜닝 손실함수: 0.038267\n","반복: 87, 파인튜닝 손실함수: 0.040216\n","반복: 88, 파인튜닝 손실함수: 0.025236\n","반복: 89, 파인튜닝 손실함수: 0.035064\n","반복: 90, 파인튜닝 손실함수: 0.052566\n","반복: 91, 파인튜닝 손실함수: 0.035644\n","반복: 92, 파인튜닝 손실함수: 0.026610\n","반복: 93, 파인튜닝 손실함수: 0.031627\n","반복: 94, 파인튜닝 손실함수: 0.033517\n","반복: 95, 파인튜닝 손실함수: 0.067333\n","반복: 96, 파인튜닝 손실함수: 0.034983\n","반복: 97, 파인튜닝 손실함수: 0.048323\n","반복: 98, 파인튜닝 손실함수: 0.016452\n","반복: 99, 파인튜닝 손실함수: 0.030347\n","반복: 100, 파인튜닝 손실함수: 0.040089\n","반복: 101, 파인튜닝 손실함수: 0.032828\n","반복: 102, 파인튜닝 손실함수: 0.023875\n","반복: 103, 파인튜닝 손실함수: 0.024164\n","반복: 104, 파인튜닝 손실함수: 0.037335\n","반복: 105, 파인튜닝 손실함수: 0.024352\n","반복: 106, 파인튜닝 손실함수: 0.029310\n","반복: 107, 파인튜닝 손실함수: 0.021850\n","반복: 108, 파인튜닝 손실함수: 0.079389\n","반복: 109, 파인튜닝 손실함수: 0.033588\n","반복: 110, 파인튜닝 손실함수: 0.024647\n","반복: 111, 파인튜닝 손실함수: 0.019989\n","반복: 112, 파인튜닝 손실함수: 0.018193\n","반복: 113, 파인튜닝 손실함수: 0.040060\n","반복: 114, 파인튜닝 손실함수: 0.029030\n","반복: 115, 파인튜닝 손실함수: 0.021877\n","반복: 116, 파인튜닝 손실함수: 0.024134\n","반복: 117, 파인튜닝 손실함수: 0.017390\n","반복: 118, 파인튜닝 손실함수: 0.032621\n","반복: 119, 파인튜닝 손실함수: 0.027420\n","반복: 120, 파인튜닝 손실함수: 0.020560\n","반복: 121, 파인튜닝 손실함수: 0.029216\n","반복: 122, 파인튜닝 손실함수: 0.015483\n","반복: 123, 파인튜닝 손실함수: 0.022872\n","반복: 124, 파인튜닝 손실함수: 0.010613\n","반복: 125, 파인튜닝 손실함수: 0.040123\n","반복: 126, 파인튜닝 손실함수: 0.015296\n","반복: 127, 파인튜닝 손실함수: 0.019505\n","반복: 128, 파인튜닝 손실함수: 0.021148\n","반복: 129, 파인튜닝 손실함수: 0.120961\n","반복: 130, 파인튜닝 손실함수: 0.020553\n","반복: 131, 파인튜닝 손실함수: 0.019131\n","반복: 132, 파인튜닝 손실함수: 0.024443\n","반복: 133, 파인튜닝 손실함수: 0.019075\n","반복: 134, 파인튜닝 손실함수: 0.028090\n","반복: 135, 파인튜닝 손실함수: 0.014464\n","반복: 136, 파인튜닝 손실함수: 0.023102\n","반복: 137, 파인튜닝 손실함수: 0.027273\n","반복: 138, 파인튜닝 손실함수: 0.014133\n","반복: 139, 파인튜닝 손실함수: 0.020516\n","반복: 140, 파인튜닝 손실함수: 0.022014\n","반복: 141, 파인튜닝 손실함수: 0.020149\n","반복: 142, 파인튜닝 손실함수: 0.030854\n","반복: 143, 파인튜닝 손실함수: 0.021076\n","반복: 144, 파인튜닝 손실함수: 0.025941\n","반복: 145, 파인튜닝 손실함수: 0.018467\n","반복: 146, 파인튜닝 손실함수: 0.025807\n","반복: 147, 파인튜닝 손실함수: 0.012198\n","반복: 148, 파인튜닝 손실함수: 0.026728\n","반복: 149, 파인튜닝 손실함수: 0.048352\n","반복: 150, 파인튜닝 손실함수: 0.016224\n","반복: 151, 파인튜닝 손실함수: 0.015653\n","반복: 152, 파인튜닝 손실함수: 0.020018\n","반복: 153, 파인튜닝 손실함수: 0.019293\n","반복: 154, 파인튜닝 손실함수: 0.014357\n","반복: 155, 파인튜닝 손실함수: 0.009002\n","반복: 156, 파인튜닝 손실함수: 0.022342\n","반복: 157, 파인튜닝 손실함수: 0.013735\n","반복: 158, 파인튜닝 손실함수: 0.015105\n","반복: 159, 파인튜닝 손실함수: 0.020378\n","반복: 160, 파인튜닝 손실함수: 0.012612\n","반복: 161, 파인튜닝 손실함수: 0.016017\n","반복: 162, 파인튜닝 손실함수: 0.042560\n","반복: 163, 파인튜닝 손실함수: 0.012918\n","반복: 164, 파인튜닝 손실함수: 0.011623\n","반복: 165, 파인튜닝 손실함수: 0.012589\n","반복: 166, 파인튜닝 손실함수: 0.012667\n","반복: 167, 파인튜닝 손실함수: 0.023336\n","반복: 168, 파인튜닝 손실함수: 0.015723\n","반복: 169, 파인튜닝 손실함수: 0.017025\n","반복: 170, 파인튜닝 손실함수: 0.015646\n","반복: 171, 파인튜닝 손실함수: 0.025437\n","반복: 172, 파인튜닝 손실함수: 0.030098\n","반복: 173, 파인튜닝 손실함수: 0.010831\n","반복: 174, 파인튜닝 손실함수: 0.034768\n","반복: 175, 파인튜닝 손실함수: 0.014551\n","반복: 176, 파인튜닝 손실함수: 0.016096\n","반복: 177, 파인튜닝 손실함수: 0.011276\n","반복: 178, 파인튜닝 손실함수: 0.014559\n","반복: 179, 파인튜닝 손실함수: 0.012053\n","반복: 180, 파인튜닝 손실함수: 0.017137\n","반복: 181, 파인튜닝 손실함수: 0.015679\n","반복: 182, 파인튜닝 손실함수: 0.015767\n","반복: 183, 파인튜닝 손실함수: 0.021577\n","반복: 184, 파인튜닝 손실함수: 0.012805\n","반복: 185, 파인튜닝 손실함수: 0.014877\n","반복: 186, 파인튜닝 손실함수: 0.017300\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TrocOWudpo8N"},"source":["오토인코더를 사용하면   \r\n","소프트맥스 분류기만 사용했을 때와 ANN을 이용했을 때보다 정확도가 더 향상된다"]},{"cell_type":"code","metadata":{"id":"S3Tkad4Ap6EB"},"source":[""],"execution_count":null,"outputs":[]}]}